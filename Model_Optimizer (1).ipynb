{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d6cf319-872a-4665-9aee-4dea65e88fa4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Project Name** : Optimization of Supply Moves (OSM) <br/>\n",
    "**Nestle SPOCs** :Aris Anadilla , Emily Knaus <br/>\n",
    "**Created By** : Akansha Rana <br/>\n",
    "**Purpose** : To Identify the Inventory that is on Risk at a DC and Recommend STOs to other DCs having demand to consume this AtRisk Inventory, to reduce the wastage <br/>\n",
    "**Date** : <br/>\n",
    "**Functionality** : This Notebook runs an optimization model based on scipy library that recommend inventory movement between two locations<br/>\n",
    "\n",
    "**Output Folder** : \n",
    "1. solutions/ift/ift/outbount/Test\n",
    "\n",
    "\n",
    "**Dependent Notebooks** : 1. Azure_Connection, 2. InputCreation_PowerApps, 3. Model_Run_Notebook_PowerApps </br>\n",
    "\n",
    "**Widgets Definition**: </br>\n",
    "<small>\n",
    "1.partition_path:partition_path will tell about the part no of the data that needs to sent throught optimizer(integer)</br>\n",
    "2.increment_value:increment value for each run(in each iteration the partition value will be increaed by this value (integer)</br>\n",
    "3.destination_path:path of folder where the data should be saved</br>\n",
    "4.dateformat: date in YYYY/MM/DD format(DataType = string)</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df9c2aef-2579-4267-8362-430ebdf68c89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Azure_Connection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7012141-71a0-4d1b-98e4-0b98f2eae9dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# importing required libraries and functions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import DateType\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.functions import current_date\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from datetime import date,datetime,timedelta\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.optimize import linprog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f934bd48-98be-4cdf-adf5-0612aaece5d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creatin parameters for the variable whose value will be passed from another notebook\n",
    "# partition_path will tell about the part no of the data that needs to sent throught optimizer\n",
    "dbutils.widgets.text(\"partition_path\", \"\", \"\")\n",
    "partition_path_value = dbutils.widgets.get(\"partition_path\")\n",
    "\n",
    "# increment value for each run\n",
    "dbutils.widgets.text(\"increment_value\", \"\", \"\")\n",
    "increment_value = dbutils.widgets.get(\"increment_value\")\n",
    "increment_value = int(increment_value)\n",
    "\n",
    "# destination value for the output\n",
    "dbutils.widgets.text(\"destination_path\", \"\", \"\")\n",
    "destination_path = dbutils.widgets.get(\"destination_path\")\n",
    "\n",
    "# dateformat variable, which tells about the Date\n",
    "dbutils.widgets.text(\"dateformat\", \"\", \"\")\n",
    "dateformat = dbutils.widgets.get(\"dateformat\")\n",
    "print(dateformat)\n",
    "ModelRunDate = datetime.strptime(dateformat, \"%Y/%m/%d\").date()  # .strftime(\"%Y-%m-%d\")\n",
    "print(partition_path_value, increment_value, destination_path, dateformat, ModelRunDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ae3654e-f53a-49b1-b279-f20c355107b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Reading Data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75e4d431-49b4-4ba0-8cd7-7a4e99f8aa5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading the AllPossibleCombinations Data From Datalake\n",
    "try:\n",
    "    AllComb = spark.read.format(\"delta\").load(\n",
    "        destination_path + \"AllPossibleCombinations\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error in loading AllPossibleCombinations data: {str(e)}\")\n",
    "    raise SystemExit(f\"Exiting due to the error: {str(e)}\")\n",
    "AllComb = AllComb.filter(sf.col(\"Report_Run_Date\") == ModelRunDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f29b9d5b-a0b0-4ae9-b9a7-3865573a51f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# here all the data is distributed in to multiple parts based on the materials. Materials are sorted in the order of their count of rows, and material count falling in a specific range is run in one loop\n",
    "sku_counts = AllComb.groupby(\"MaterialID\").count()\n",
    "sku_counts = sku_counts.withColumn(\n",
    "    \"cum_sum\",\n",
    "    sf.sum(\"count\").over(\n",
    "        Window.orderBy(\"count\", \"MaterialID\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "    ),\n",
    ")\n",
    "# filtering material for the specific range of count based on the partition value\n",
    "sku_counts = sku_counts.filter(\n",
    "    (sf.col(\"cum_sum\") <= (int(partition_path_value) + increment_value))\n",
    "    & (sf.col(\"cum_sum\") > int(partition_path_value))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbf40118-2059-411f-85a7-41847ce120e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# selecting the material for inscope based on sku_counts\n",
    "AllComb = AllComb.join(sku_counts.select(\"MaterialID\"), [\"MaterialID\"])\n",
    "\n",
    "# if there is no material present in any specific range, there is no need to run the notebook further, so exiting\n",
    "row_count = AllComb.count()\n",
    "if row_count <= 0:\n",
    "    dbutils.notebook.exit(\"No Material Present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5690df5-571c-4853-947c-3be8417cb307",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# converting the required columns to a pandas dataframe\n",
    "AllComb_pd_cpy = AllComb.select(\n",
    "    \"MaterialID\",\n",
    "    \"Source\",\n",
    "    \"SalvageDate\",\n",
    "    \"QuantityinUoM\",\n",
    "    \"Destination\",\n",
    "    \"Date\",\n",
    "    \"ExcessDemand\",\n",
    "    \"Min_Of_Upcoming_Days\",\n",
    "    \"arrivaldate\",\n",
    "    \"IntermediateDemandColumn\",\n",
    "    \"ActualExcessDemand\",\n",
    "    \"source_dest_sku_salvagedate\",\n",
    "    \"source_sku_salvagedate\",\n",
    "    \"source_sku_salvagedate_date\",\n",
    "    \"source_dest_sku_salvagedate_arrivaldate\",\n",
    "    \"source_dest_arrivaldate\",\n",
    "    \"source_dest_dispatchDate\",\n",
    "    \"dest_sku\",\n",
    "    \"dest_sku_date\",\n",
    "    \"dest_sku_date_arrivaldate\",\n",
    "    \"OBJ\",\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "415f4280-27b2-4710-a77b-cbb3986778cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# list of all distinct combinations\n",
    "source_dest_sku_salvagedate_arrivaldate_list = AllComb_pd_cpy[\n",
    "    \"source_dest_sku_salvagedate_arrivaldate\"\n",
    "].tolist()\n",
    "len(source_dest_sku_salvagedate_arrivaldate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b43a4d91-aacc-4b80-8de0-ba4e009f448f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# performing onHotEncoding and creatinf a dummy Identity matrix that will tell about the selection of any combination for the recommendation\n",
    "AllComb_pd_cpy[\"srno\"] = AllComb_pd_cpy.index + 1\n",
    "AllComb_pd_cpy[\"min_inv\"] = AllComb_pd_cpy[[\"QuantityinUoM\", \"ActualExcessDemand\"]].min(\n",
    "    axis=1\n",
    ")\n",
    "rated_dummies = pd.get_dummies(AllComb_pd_cpy.srno)\n",
    "AllComb_pd_cpy = pd.concat([AllComb_pd_cpy, rated_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc1cfd6e-e02b-484e-ae9c-cdd30d9394b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "column_names = [\n",
    "    i for i in range(1, len(source_dest_sku_salvagedate_arrivaldate_list) + 1)\n",
    "]\n",
    "\n",
    "#defining objective function\n",
    "# the objective function is to maximize AllComb_pd_cpy['OBJ'], but with scipy it can only be minimized ,so we will minimize AllComb_pd_cpy['OBJ'] * -1 and get our work done\n",
    "obj = np.array(AllComb_pd_cpy[\"OBJ\"] * -1)\n",
    "lhs = np.array(AllComb_pd_cpy[column_names])\n",
    "qty_movable_source_dest_sku_salvagedate_arrivaldate = AllComb_pd_cpy[\"min_inv\"]\n",
    "rhs = np.array(AllComb_pd_cpy[\"min_inv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afe33220-8689-4651-af51-30ba71f803fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# first part of constraint. source_sku_salvahedate level moved qty should be less than the available qty\n",
    "source_sku_salvagedate_movedfrom = AllComb_pd_cpy.groupby(\"source_sku_salvagedate\").agg(\n",
    "    {\"QuantityinUoM\": \"mean\"}\n",
    ")\n",
    "source_sku_salvagedate_movedfrom1 = AllComb_pd_cpy.groupby(\"source_sku_salvagedate\")[\n",
    "    column_names\n",
    "].apply(lambda x: x.astype(int).sum())\n",
    "source_sku_salvagedate_movedfrom = source_sku_salvagedate_movedfrom.join(\n",
    "    source_sku_salvagedate_movedfrom1, on=\"source_sku_salvagedate\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b24d184-024f-46bc-963f-ed08621da68a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Adding constraint in the model\n",
    "lhs1 = np.concatenate(\n",
    "    (lhs, np.array(source_sku_salvagedate_movedfrom[column_names])), axis=0\n",
    ")\n",
    "rhs1 = np.concatenate(\n",
    "    (rhs, np.array(source_sku_salvagedate_movedfrom[\"QuantityinUoM\"])), axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1508667-0330-4f52-b6d6-04f977c04937",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "del source_sku_salvagedate_movedfrom, source_sku_salvagedate_movedfrom1, lhs, rhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eab0e662-e623-44a4-b1fb-14a3aba2df1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# second part of constraint. dest_sku_date level moved qty should be less than the usable demand\n",
    "dest_sku_date_movedto1 = (\n",
    "    AllComb_pd_cpy.sort_values(by=\"dest_sku_date_arrivaldate\")\n",
    "    .groupby([\"Destination\", \"MaterialID\", \"Date\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"ExcessDemand\": \"mean\",\n",
    "            \"IntermediateDemandColumn\": \"mean\",\n",
    "            \"Min_Of_Upcoming_Days\": \"mean\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "dest_sku_date_movedto2 = (\n",
    "    AllComb_pd_cpy.sort_values(by=\"dest_sku_date_arrivaldate\")\n",
    "    .groupby([\"Destination\", \"MaterialID\", \"Date\"])[column_names]\n",
    "    .apply(lambda x: x.astype(int).sum())\n",
    ")\n",
    "\n",
    "dest_sku_date_movedto2 = (\n",
    "    dest_sku_date_movedto2.sort_values(\n",
    "        by=[\"Destination\", \"MaterialID\", \"Date\"], ascending=True\n",
    "    )\n",
    "    .groupby([\"Destination\", \"MaterialID\"])[column_names]\n",
    "    .apply(lambda x: x.cumsum())\n",
    ")\n",
    "\n",
    "\n",
    "dest_sku_date_movedto = dest_sku_date_movedto1.join(\n",
    "    dest_sku_date_movedto2, on=[\"Destination\", \"MaterialID\", \"Date\"]\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7333e415-a505-4ec0-9557-38a0e7288539",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Adding constraint in the model\n",
    "lhs2 = np.concatenate((lhs1, np.array(dest_sku_date_movedto[column_names])), axis=0)\n",
    "rhs2 = np.concatenate(\n",
    "    (rhs1, np.array(dest_sku_date_movedto[\"Min_Of_Upcoming_Days\"])), axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f353ace9-2e6f-483e-8d7b-103a9b34f630",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "del dest_sku_date_movedto, lhs1, rhs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11aff512-66e7-49be-8d60-883318a531e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3rd part - second part of constraint. dest_sku_date_arrivaldate level moved qty should be less than the usable demand\n",
    "dest_sku_date_arrival_movedto1 = (\n",
    "    AllComb_pd_cpy.sort_values(by=\"dest_sku_date_arrivaldate\", ascending=False)\n",
    "    .groupby([\"Destination\", \"MaterialID\", \"Date\", \"arrivaldate\"])\n",
    "    .agg({\"ExcessDemand\": \"mean\", \"IntermediateDemandColumn\": \"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "dest_sku_date_arrival_movedto1 = dest_sku_date_arrival_movedto1.sort_values(\n",
    "    by=[\"Destination\", \"MaterialID\", \"Date\", \"arrivaldate\"], ascending=False\n",
    ")\n",
    "dest_sku_date_arrival_movedto1[\"MovableQty\"] = (\n",
    "    dest_sku_date_arrival_movedto1[\"ExcessDemand\"]\n",
    "    - dest_sku_date_arrival_movedto1[\"IntermediateDemandColumn\"]\n",
    ")\n",
    "dest_sku_date_arrival_movedto1.loc[\n",
    "    dest_sku_date_arrival_movedto1[\"MovableQty\"] <= 0, \"MovableQty\"\n",
    "] = 0\n",
    "\n",
    "dest_sku_date_arrival_movedto2 = (\n",
    "    AllComb_pd_cpy.sort_values(by=\"dest_sku_date_arrivaldate\", ascending=False)\n",
    "    .groupby([\"Destination\", \"MaterialID\", \"Date\", \"arrivaldate\"])[column_names]\n",
    "    .apply(lambda x: x.astype(int).sum())\n",
    ")  # .reset_index()\n",
    "\n",
    "dest_sku_date_arrival_movedto2 = (\n",
    "    dest_sku_date_arrival_movedto2.sort_values(\n",
    "        by=[\"Destination\", \"MaterialID\", \"Date\", \"arrivaldate\"], ascending=False\n",
    "    )\n",
    "    .groupby([\"Destination\", \"MaterialID\", \"Date\"])[column_names]\n",
    "    .apply(lambda x: x.cumsum())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cea2ddb6-b0cf-4f97-b56c-12c06db414fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Adding constraint in the model\n",
    "lhs3 = np.concatenate(\n",
    "    (lhs2, np.array(dest_sku_date_arrival_movedto2[column_names])), axis=0\n",
    ")\n",
    "rhs3 = np.concatenate(\n",
    "    (rhs2, np.array(dest_sku_date_arrival_movedto1[\"MovableQty\"])), axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "683a85e4-0284-429a-896d-3a6ad7f22f34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "del dest_sku_date_arrival_movedto1, dest_sku_date_arrival_movedto2, lhs2, rhs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54690fc8-99e7-4b89-af7d-c3166376321d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4th part when there are multiple batchs recommended at one destination for one material, at each date and arrival date combination sum of all previous recommended qty should not exceed the usable excess demand\n",
    "All_ArrivalDates = AllComb_pd_cpy[\n",
    "    [\"Destination\", \"MaterialID\", \"arrivaldate\"]\n",
    "].drop_duplicates()\n",
    "All_DemandDates = AllComb_pd_cpy[\n",
    "    [\"Destination\", \"MaterialID\", \"Date\"]\n",
    "].drop_duplicates()\n",
    "\n",
    "All_DemandDates_ArrivalDates = All_DemandDates.merge(\n",
    "    All_ArrivalDates, on=[\"Destination\", \"MaterialID\"]\n",
    ")\n",
    "All_DemandDates_ArrivalDates = All_DemandDates_ArrivalDates[\n",
    "    All_DemandDates_ArrivalDates[\"Date\"] > All_DemandDates_ArrivalDates[\"arrivaldate\"]\n",
    "]\n",
    "\n",
    "All_DemandDates_ArrivalDates = All_DemandDates_ArrivalDates.merge(\n",
    "    AllComb_pd_cpy[[\"Destination\", \"MaterialID\", \"Date\", \"arrivaldate\"] + column_names],\n",
    "    on=[\"Destination\", \"MaterialID\"],\n",
    ")\n",
    "\n",
    "All_DemandDates_ArrivalDates = All_DemandDates_ArrivalDates[\n",
    "    All_DemandDates_ArrivalDates[\"Date_x\"] >= All_DemandDates_ArrivalDates[\"Date_y\"]\n",
    "]\n",
    "\n",
    "All_DemandDates_ArrivalDates = All_DemandDates_ArrivalDates[\n",
    "    All_DemandDates_ArrivalDates[\"arrivaldate_x\"]\n",
    "    <= All_DemandDates_ArrivalDates[\"arrivaldate_y\"]\n",
    "]\n",
    "\n",
    "All_DemandDates_ArrivalDates = (\n",
    "    All_DemandDates_ArrivalDates.groupby(\n",
    "        [\"Destination\", \"MaterialID\", \"Date_x\", \"arrivaldate_x\"]\n",
    "    )[column_names]\n",
    "    .apply(lambda x: x.sum())\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "Excess_demand_intermediate_column = AllComb_pd_cpy[\n",
    "    [\n",
    "        \"Destination\",\n",
    "        \"MaterialID\",\n",
    "        \"Date\",\n",
    "        \"arrivaldate\",\n",
    "        \"Min_Of_Upcoming_Days\",\n",
    "        \"IntermediateDemandColumn\",\n",
    "    ]\n",
    "].drop_duplicates()\n",
    "All_DemandDates_ArrivalDates.rename(\n",
    "    columns={\"Date_x\": \"Date\", \"arrivaldate_x\": \"arrivaldate\"}, inplace=True\n",
    ")\n",
    "\n",
    "All_DemandDates_ArrivalDates = All_DemandDates_ArrivalDates.merge(\n",
    "    Excess_demand_intermediate_column,\n",
    "    on=[\"Destination\", \"MaterialID\", \"Date\", \"arrivaldate\"],\n",
    ")\n",
    "\n",
    "All_DemandDates_ArrivalDates[\"MovableQty2\"] = (\n",
    "    All_DemandDates_ArrivalDates[\"Min_Of_Upcoming_Days\"]\n",
    "    - All_DemandDates_ArrivalDates[\"IntermediateDemandColumn\"]\n",
    ")\n",
    "All_DemandDates_ArrivalDates.loc[\n",
    "    All_DemandDates_ArrivalDates[\"MovableQty2\"] <= 0, \"MovableQty2\"\n",
    "] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fa86cca-b5a7-4232-9f13-95540b2a391c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Adding constraint in the model\n",
    "lhs31 = np.concatenate(\n",
    "    (lhs3, np.array(All_DemandDates_ArrivalDates[column_names])), axis=0\n",
    ")\n",
    "rhs31 = np.concatenate(\n",
    "    (rhs3, np.array(All_DemandDates_ArrivalDates[\"MovableQty2\"])), axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a372bb4-32dc-4308-b3dd-2d47bb2d8b5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "del All_DemandDates_ArrivalDates, Excess_demand_intermediate_column, lhs3, rhs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1052ebc-dfad-463a-b12c-f914ee241f93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5th part- for any arrival date, qty recommended should not exceed the intermediate demand(between date and arrival date) and the minimum of upcoming days\n",
    "dest_sku_arrival_date_movedto1 = (\n",
    "    AllComb_pd_cpy.sort_values(by=\"dest_sku_date_arrivaldate\", ascending=True)\n",
    "    .groupby([\"Destination\", \"MaterialID\", \"Date\", \"arrivaldate\"])\n",
    "    .agg({\"ExcessDemand\": \"mean\", \"IntermediateDemandColumn\": \"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "dest_sku_arrival_date_movedto1 = dest_sku_arrival_date_movedto1.sort_values(\n",
    "    by=[\"Destination\", \"MaterialID\", \"arrivaldate\", \"Date\"], ascending=True\n",
    ")\n",
    "dest_sku_arrival_date_movedto1[\"MovableQty1\"] = (\n",
    "    dest_sku_arrival_date_movedto1[\"ExcessDemand\"]\n",
    "    - dest_sku_arrival_date_movedto1[\"IntermediateDemandColumn\"]\n",
    ")\n",
    "dest_sku_arrival_date_movedto1.loc[\n",
    "    dest_sku_arrival_date_movedto1[\"MovableQty1\"] <= 0, \"MovableQty1\"\n",
    "] = 0\n",
    "\n",
    "dest_sku_arrival_date_movedto2 = (\n",
    "    AllComb_pd_cpy.sort_values(\n",
    "        by=[\"Destination\", \"MaterialID\", \"arrivaldate\", \"Date\"], ascending=True\n",
    "    )\n",
    "    .groupby([\"Destination\", \"MaterialID\", \"arrivaldate\", \"Date\"])[column_names]\n",
    "    .apply(lambda x: x.astype(int).sum())\n",
    ")  # .reset_index()\n",
    "\n",
    "dest_sku_arrival_date_movedto2 = (\n",
    "    dest_sku_arrival_date_movedto2.sort_values(\n",
    "        by=[\"Destination\", \"MaterialID\", \"arrivaldate\", \"Date\"], ascending=True\n",
    "    )\n",
    "    .groupby([\"Destination\", \"MaterialID\", \"arrivaldate\"])[column_names]\n",
    "    .apply(lambda x: x.cumsum())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77045f91-86e6-4205-b660-8c9422f955f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Adding constraint in the model\n",
    "lhs4 = np.concatenate(\n",
    "    (lhs31, np.array(dest_sku_arrival_date_movedto2[column_names])), axis=0\n",
    ")\n",
    "rhs4 = np.concatenate(\n",
    "    (rhs31, np.array(dest_sku_arrival_date_movedto1[\"MovableQty1\"])), axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0726a87-fc5f-4aa0-aa1f-508ba2f98776",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "del dest_sku_arrival_date_movedto1, dest_sku_arrival_date_movedto2, lhs31, rhs31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d83c26f0-49da-4995-892a-05f886c612b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4afe2ae5-9aba-4597-b08e-15f7c5c6d773",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# shape of the matrix\n",
    "print(obj.shape, lhs4.shape, rhs4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c7145d6-48c3-4300-a912-4437f318b34e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# running the solver\n",
    "\n",
    "lpout = linprog(c=obj, A_ub=lhs4, b_ub=rhs4, method=\"highs\")\n",
    "lpout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c758d93b-e1f0-42f4-b74b-60c171b48a4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# x1 is solution, that will tell about the qty\n",
    "x1 = lpout.x\n",
    "len(x1)\n",
    "# creating a dataframe using X1\n",
    "x3 = pd.DataFrame(x1, columns=[\"QtyMoved\"])\n",
    "# joining the putput with All comb\n",
    "out = pd.concat([AllComb_pd_cpy, x3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0e10fe3-2eda-42bd-8cd8-666095230372",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# selecting some columns for the final output\n",
    "# if there are multiple arrival dates for one lane, earliest arrival date will be selected for the lane\n",
    "out1 = out[\n",
    "    [\"MaterialID\", \"Source\", \"Destination\", \"SalvageDate\", \"arrivaldate\", \"QtyMoved\"]\n",
    "]\n",
    "STO_qty = spark.createDataFrame(out1)\n",
    "STO_qty = (\n",
    "    STO_qty.filter(sf.col(\"QtyMoved\") > 0)\n",
    "    .groupby(\"MaterialID\", \"Source\", \"Destination\", \"SalvageDate\")\n",
    "    .agg(\n",
    "        sf.min(\"arrivaldate\").alias(\"arrivaldate\"), sf.sum(\"QtyMoved\").alias(\"QtyMoved\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d33f6ee5-4902-438b-a2dc-ca9137ba33a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the DataFrame in file on ADLS\n",
    "STO_qty = (\n",
    "    STO_qty.withColumn(\"partition_value\", sf.lit(partition_path_value))\n",
    "    .withColumn(\"Report_Run_Date\", sf.lit(ModelRunDate))\n",
    "    .withColumn(\"Year\", sf.substring(\"Report_Run_Date\", 1, 4))\n",
    "    .withColumn(\"Month\", sf.substring(\"Report_Run_Date\", 6, 2))\n",
    "    .withColumn(\"Day\", sf.substring(\"Report_Run_Date\", 9, 2))\n",
    ")\n",
    "\n",
    "STO_qty.write.format(\"delta\").mode(\"append\").option(\"overwriteSchema\", \"true\").option(\n",
    "    \"mergeSchema\", \"true\"\n",
    ").partitionBy(\"Year\", \"Month\", \"Day\").save(destination_path + \"/SalvageDateLevelOutput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9668474d-ab06-4018-94c4-8fa27aea1361",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit('continue')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Model_Optimizer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
